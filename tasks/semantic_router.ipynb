{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from happy_hub.models import OllamaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaModel(\n",
    "    url=\"http://34.1.198.45:11434/v1\",\n",
    "    model_config_dict={\n",
    "        \"model\": \"gemma2:27b\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"stream\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.Stream at 0x7f86f6259d30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.run(messages=[\n",
    "    {\"role\": \"user\", \"content\": \"xin chào\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../happy_hub/prompts/semantic_router.yml\", 'r') as file:\n",
    "    semantic_router_yaml = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_template = \"\"\n",
    "for key in semantic_router_yaml:\n",
    "    system_prompt_template += semantic_router_yaml[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from happy_hub.functions import RETRIEVAL_FUNCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"mb_info_query\"}"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\"role\": \"user\", \"content\": \"xin chào\"},\n",
    "    {\"role\": \"assistant\",  'content': '{\"name\": \"fun_chat\"}'},\n",
    "    {\"role\": \"user\", \"content\": \"quy định nghỉ phép của MB gồm những gì?\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\": \"mb_info_query\"}'},\n",
    "    {\"role\": \"user\", \"content\": \"MB có gì hay ho nhỉ ?\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\": \"mb_info_query\"}'},\n",
    "]\n",
    "\n",
    "system_prompt = system_prompt_template.format(\n",
    "    tools = [func.get_openai_tool_schema() for func in RETRIEVAL_FUNCS],\n",
    ")\n",
    "\n",
    "# Prepare the input messages\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": system_prompt},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\": \"mb_info_query\"}'}\n",
    "]\n",
    "\n",
    "messages.extend(examples)\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"user\", \"content\": \"làm nhân viên chính thức có được giãm lãi vay vốn\"\n",
    "})\n",
    "\n",
    "# Run the model with the input messages\n",
    "response = model.run(messages=messages) # type: ignore\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../happy_hub/prompts/fun_chat.yml\", 'r') as file:\n",
    "    fun_chat_yaml = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_chat_template = \"\"\n",
    "for key in fun_chat_yaml:\n",
    "    fun_chat_template += fun_chat_yaml[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m fun_chat_messages\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtôi muốn đi chơi hụi để giàu nhanh\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m })\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Run the model with the input messages\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mrun(messages\u001b[38;5;241m=\u001b[39mfun_chat_messages) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdelta\u001b[38;5;241m.\u001b[39mcontent, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare the input messages\n",
    "fun_chat_messages = [\n",
    "    {\"role\": \"user\", \"content\": fun_chat_template},\n",
    "    {\"role\": \"assistant\", \"content\": 'Rất vui được giúp đỡ bạn, mình là happy hub!'}\n",
    "]\n",
    "\n",
    "fun_chat_messages.append({\n",
    "    \"role\": \"user\", \"content\": \"tôi muốn đi chơi hụi để giàu nhanh\"\n",
    "})\n",
    "\n",
    "# Run the model with the input messages\n",
    "response = model.run(messages=fun_chat_messages) # type: ignore\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MB Info Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../happy_hub/prompts/mb_info_query.yml\", 'r') as file:\n",
    "    mb_info_query_yaml = yaml.safe_load(file)\n",
    "\n",
    "mb_info_query_template = \"\"\n",
    "for key in mb_info_query_yaml:\n",
    "    mb_info_query_template += mb_info_query_yaml[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from happy_hub.storage import SimpleVectorDB\n",
    "\n",
    "vector_db = SimpleVectorDB()\n",
    "vector_db.load_from_file(\"../vector_db.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_qa_text(text):\n",
    "    \"\"\"\n",
    "    Parses a given text into a dictionary with 'question' and 'answer' keys.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing a question and answer separated by ';'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with 'question' and 'answer' as keys.\n",
    "\n",
    "    Example:\n",
    "        text = \"question: Quy định về chế độ nghỉ phép có hưởng lương tại MB?; answer: Bạn có thời gian làm việc...\"\n",
    "        result = parse_qa_text(text)\n",
    "    \"\"\"\n",
    "    # Split the text into question and answer parts\n",
    "    parts = text.split(';')\n",
    "    \n",
    "    # Initialize the result dictionary\n",
    "    result = {}\n",
    "    \n",
    "    # Parse the question\n",
    "    if parts[0].startswith('question:'):\n",
    "        result['question'] = parts[0].split('question:', 1)[1].strip()\n",
    "    \n",
    "    # Parse the answer\n",
    "    if len(parts) > 1 and parts[1].startswith(' answer:'):\n",
    "        result['answer'] = parts[1].split(' answer:', 1)[1].strip()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'utter_ket_thuc', 'answer': 'Happy Hub hẹn gặp lại và chúc bạn một ngày làm việc suôn sẻ'}\n",
      "------------------------------\n",
      "{'question': 'Chào hỏi', 'answer': 'Chuyện gì khó, có Happy Hub lo!!! Hãy để mình hỗ trợ bạn nhé'}\n",
      "------------------------------\n",
      "{'question': 'Chào hỏi', 'answer': 'Chào bạn, mình là Happy Hub rất vui khi được hỗ trợ bạn. Bạn đang quan tâm tới vấn đề gì? Vui lòng cho mình biết để bên mình hỗ trợ nhé!'}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "results = vector_db.search(\"ôi chao ôi tùng\", top_k=3)\n",
    "document = []\n",
    "for doc, score in results:\n",
    "    print(parse_qa_text(doc))\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_mb_info_query(query: str):\n",
    "    import json\n",
    "    results = vector_db.search(query=query, top_k=5)\n",
    "    documents = []\n",
    "    for doc, score in results:\n",
    "        documents.append(str(parse_qa_text(doc)))\n",
    "    system_prompt = mb_info_query_template.format(documents = '\\n-----------------------------\\n'.join(documents))\n",
    "    \n",
    "    # print(system_prompt)\n",
    "    \n",
    "    mb_query_messages = [\n",
    "        {\"role\": \"user\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    response = model.run(mb_query_messages) # type: ignore\n",
    "    for chunk in response:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rất tiếc, hiện tại mình không có đủ thông tin chính xác để trả lời câu hỏi của bạn một cách đầy đủ. Để có được thông tin chi tiết và chính xác nhất, bạn hãy liên hệ với bộ phận nhân sự của MB tại abc.xyz \n"
     ]
    }
   ],
   "source": [
    "answer_mb_info_query(\"chương trình tín dụng xanh ở MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
